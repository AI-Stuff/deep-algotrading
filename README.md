#A tour through tensorflow with financial data

I present several models ranging in complexity from simple regression to LSTM and policy networks. The series can be used as an educational resource for tensorflow or deep learning, a reference aid, or a source of ideas on how to apply deep learning techniques to problems that are outside of the usual deep learning fields (vision, natural language).

Not all of the examples will work. Some of them are far to simple to even be considered viable trading strategies and are only presented for educational purposes. Others, in the notebook form I present, have not been trained for the proper amount of time. Perhaps with a bit of rented GPU time they will be more promising and I leave that as an excercise for the reader (who wants to make a lot of money). Hopefully this project inspires some to try using deep learning techniques for some more interesting problems. [Contact me](<ljrconnell@gmail.com>) if interested in learning more or if you have suggestions for additions or improvements. 

The algorithms increase in complexity and introduce new concepts as they progress:

1. [Simple Regression][1]: Here we regress the prices from the last 100 days to the next day's price, training *W* and *b* in the equation *y = Wx + b* where *y* is the next day's price, *x* is a vector of dimension 100, *W* is a 100x1 matrix and *b* is a 1x1 matrix. We run the gradient descent algorithm to minimize the mean squared error of the predicted price and the actual next day price. Congratulations you passed highschool stats. But hopefully this simple and naive example helps demonstrate the idea of a tensor graph, as well as showing a great example of extreme overfitting. 
2. [Simple Regression on Multiple Symbols][2]: Things get a little more interesting as soon as we introduce more than one symbol. What is the best way to model our eventual investment strategy: our policy, if you will. We start to realize that our model only vaguely implies a policy (investment actions) by predicting the actual movement in price. The implied policy is simple: buy if the the predicted price movement is positive, sell if it is negative. But that doesnt sound realistic at all. How much do we buy? And will optimizing this, even if we are very careful to avoid overfitting, even produce results that allign with our goals? We havent actaully defined our goals explicitly, but for those who are not familiar with investment metrics, common goals include:
    + maximize risk adjusted return (like the [Sharpe](https://en.wikipedia.org/wiki/Sharpe_ratio) ratio)
    + consistency of returns over time
    + low market exposure
    + [long/short equity](http://www.investopedia.com/terms/l/long-shortequity.asp) 

 If markets were easy to figure out and we could accurately predict the next day's return then it wouldn't matter. Our implied policy would fit with some goals (not long/short equity though) and the strategy would be viable. The reality is that our model cannot accurately predict this, nor will our strategy ever be perfect. Our best case scenario is always winning slightly more than losing. When operating on these margins it is much more important that we consider the policy explicitly, thus moving to 'Policy Based' deep learning. 
3. [Policy Gradient Training][3]: Our policy will remain simple. We will chose a position, long/neutral/short, for each symbol in our portfolio. But now, instead of letting our estimation of the future return inform our decision, we train our network to choose the best position. Thus, instead of having an *implied* policy, it is *explicit* and *trained* directly. 
 Even thought the policy is simple in this case, training it is a bit more involved. I did my best to interpret Andrej Karpathy's excelent article on [Reinforcement Learning](http://karpathy.github.io/2016/05/31/rl/) when writing this code. It might be worth reading his explanation, but I'll do my best to summarize what I did.
 
 For each symbol in our portfolio, we **sample** (or argmax if the math is too hard) the probability distribution of our three position buckets to get our policy decision (a position l/s/n), we multiply our position by the target value to get a daily return for the symbol. Then we combine the symbols to get a full daily return. We can also get other metrics like the total return and sharpe ratio since we actually are feeding this through as a batch (more on that later). As Karpathy points out, we are *only interested in the gradients of the positions we sampled*, so we select the appropriate columns from the output and combine them into a new tensor.
 
 So now we have a tensor with the regression's probability for the chosen (sampled) action for each symbol and each day. We also have a few performance metrics like daily and total return to choose from, but they're not differentiable because we sampled the probability so we cant just "gradient descent maximize" the profit...unfortunately. Instead, we find the cross entropy between the first table (the probabilities we chose/sampled) and an all-ones tensor of the same shape. We get a table of cross entropies of the same size (number of symbols by batch size) This is basically equivalent to saying, how do I do MORE of what I'm already doing, for every decision that I made. Now we dont necessarilly want MORE of what we're doing, but the opposite of it is definitely LESS of it, which is useful. We multiply that tensor by our fitness function (the daily or aggregate return) and we use the gradient descent optimizer to minimize the cost. So you see? If the fitness function is negative, it will train the weights of the regression to NOT do what it just did. Its a pretty cool idea and it can be applied to a lot of problems that are much more interesting. I give some examples in the notebook about which different fitness functions you can apply which I think is better explained by seeing it. 
4. [Stochastic Gradient Descent][4]: As you saw in the notebook, the policy gradient doesnt train very well when we are grading it on the return over the entire dataset, but it trains very well when it uses each day's return or the position on each symbol every day. This makes sense, if we just take the total return over several years and its slightly positive then we tell our machine to do more of that. That will do almost nothing since so many of those decisions were actually losing money. The problem, as we have it set up now, needs to be broken down into smaller units. Fortunately there is some mathematical proof that that is legal and even faster. Score! 

 Stochastic Gradient Descent is basically just breaking your data into smaller batches and doing gradient descent on it. It will have slightly less accurate gradients WRT to the entire dataset's cost function, but since you are able to iterate faster with smaller batches you can run way more of them. There might even be more advantages to SGD that I'm not even mentioning, why dont you read the wikipedia or arXiv on it instead of taking my information? Or just use it since it works. If you're going on a wikipedia learning binge you might as well also learn about Momentum and Adagrad. They exist and they are more efficient but they're only really useful for people doing much bigger projects. If you are working on a huge project and your twobucksanhour AWS GPU instance is too slow then you should definitely be using them (and not be reading this introductory tutorial).


```By now you're probably wondering "does this guy even know what deep learning is? I havent seen a single neural network!" Well, I wanted to talk about other things before neural networks to show how much tensorflow can be used for before neural networks even get mentioned. Tensorflow makes neural nets so easy that you barely even notice they're part of the picture and its definitely not worth getting bogged down by their math if you dont have a solid understanding of the math behind cross entropy and the like. They can confuse ideas like policy gradients. Maybe I'll try to get a regression to play pong so that everyone shuts about neural networks...```

Stay tuned for some articles that I will write about the algorithms used here and a discussion of the difficulties of using these techniques for algorithmic trading developement.  

[1]: /notebooks/singlestock_regresion_(1).ipynb
[2]: /notebooks/multistock_regresion_(2).ipynb
[3]: /notebooks/regression_with_policy_training_(3).ipynb
[4]: /notebooks/stochastic_gradient_descent_(4).ipynb
